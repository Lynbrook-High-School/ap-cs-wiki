---
title: Sorting Objects
date: '2025-1-11'
tags: ['Ch 13 Sorting Searching']
draft: false
summary: How to sort an list of objects.
---

In most practical scenarios, we won't be writing our own sorting function.
Instead, we would use Java's built in sort functions.

For a collection (e.g. an `ArrayList`), we use `java.util.Collections.sort()`.

```
Collections.sort(container);
```

For a primitive array, we use `java.util.Arrays.sort()`.

```
Arrays.sort(arr);
```

However, in the context of AP CS, we still need to know a few sorting algorithms.
These include but are not limited to:
- Selection Sort
- Insertion Sort
- Merge Sort
- Quick Sort

## Selection Sort

The rough idea is that we select the smallest
element from our current (unsorted) list, and place that
element at the beginning of the unsorted
list.

You can find an animation of how this works [here](https://www.w3schools.com/dsa/dsa_algo_selectionsort.php).

```java
public static void selectionSort(int[] arr) {
    int n = arr.length;
    for (int i = 0; i < n; i++) {
        int best = i;
        for (int j = i + 1; j < n; j++) {
            if (arr[best] > arr[j]) {
                best = j;
            }
        }

        int temp = arr[i];
        arr[i] = arr[best];
        arr[best] = temp;
    }
}
```

| Case             | Time Complexity       | Explanation                                           |
|------------------|-----------------------|-------------------------------------------------------|
| Best Case        | $\mathcal{O}(n^{2})$  | Still compares all elements.                         |
| Average Case     | $\mathcal{O}(n^{2})$  | Average comparisons are dominated by $\mathcal{O}(n^{2})$. |
| Worst Case       | $\mathcal{O}(n^{2})$  | Same as average case; scans all elements.            |
| Space Complexity | $\mathcal{O}(1)$      | In-place sorting.                                     |

## Insertion Sort

Here, we maintain two separate arrays: one of
them being sorted, and the other one being
unsorted.

Our sorted array is initially empty, and our unsorted one is our initial list. Then, we 
iterate through each element in our unsorted
list, and insert it into its correct position
in the sorted list.

You can find an animatino of how this works
[here](https://www.w3schools.com/dsa/dsa_algo_insertionsort.php).

```java
public static void insertionSort(int[] arr) {
    int n = arr.length;
    for (int i = 1; i < n; i++) {
        int j = i - 1;
        while (j >= 0 && arr[j] > arr[j + 1]) {
            int temp = arr[j + 1];
            arr[j + 1] = arr[j];
            arr[j] = temp;
            j--;
        }
    }
}
```

Note that the actual implementation does not require two separate lists; it's
just a tad easier to think of it that way.

| Case             | Time Complexity       | Explanation                                           |
|------------------|-----------------------|-------------------------------------------------------|
| Best Case        | $\mathcal{O}(n)$      | Only $n-1$ comparisons needed when sorted.            |
| Average Case     | $\mathcal{O}(n^{2})$  | Compares with about half of the sorted portion.       |
| Worst Case       | $\mathcal{O}(n^{2})$  | Requires full comparisons when reversed.              |
| Space Complexity | $\mathcal{O}(1)$      | In-place sorting.                                     |

## Merge Sort

We are now moving on to our first "fast" sorting algorithm. Merge sort, unlike 
the previous sorts, is a **divide and conquer** algorithm. That is, we divide 
the problem into two smaller parts, and then combine our answers.

The rough procedure goes as follows:
1. Find the midpoint in our array
2. Recursively sort the left and right halves of the array
3. Merge the left and right halves in $\mathcal{O}(n)$, with the final array being sorted

To do step 3, we use the fact that the left and right halfs are sorted to our
advantage. We create a new list, and use the following procedure while we 
still have elements to add to the list.
- Compare the first available elements in the left and right halves
- Add the smaller element to the back of our list

If one list is empty, we add all the contents of the other list to the back of
our resulting list.

Note that this sorting algorithm is really good for data structures without 
random access (e.g. linked lists) because the merging step only requires us to
know the first element in the left and right halves.

```java
public static void mergeSort(int[] arr, int l, int r) {
    if (l >= r) { return; }
    int mid = (l + r) / 2;
    mergeSort(arr, l, mid);
    mergeSort(arr, mid + 1, r);

    // if the array is already sorted, we don't need to do anything
    if (arr[mid] <= arr[mid + 1]) {
        return;
    }

    int[] left = new int[mid - l + 1];
    for (int i = l; i <= mid; i++) {
        left[i - l] = arr[i];
    }

    int[] right = new int[r - mid];
    for (int i = mid + 1; i <= r; i++) {
        right[i - (mid + 1)] = arr[i];
    }

    int leftIdx = 0;
    int rightIdx = 0;
    for (int i = l; i <= r; i++) {
        if (leftIdx == left.length) {
            arr[i] = right[rightIdx];
            rightIdx++;
        } else if (rightIdx == right.length) {
            arr[i] = left[leftIdx];
            leftIdx++;
        } else {
            if (left[leftIdx] <= right[rightIdx]) {
                arr[i] = left[leftIdx];
                leftIdx++;
            } else {
                arr[i] = right[rightIdx];
                rightIdx++;
            }
        }
    }
}
```

| Case             | Time Complexity        | Explanation                                           |
|------------------|------------------------|-------------------------------------------------------|
| Best Case        | $\mathcal{O}(n)$        | Simple check if already sorted.                       |
| Average Case     | $\mathcal{O}(n \log n)$ | Consistent division and merging.                      |
| Worst Case       | $\mathcal{O}(n \log n)$ | Same as average case, always splits and merges.       |
| Space Complexity | $\mathcal{O}(n)$        | Needs extra space for merging.                        |


Understanding why this algorithm works in $\mathcal{O}(n \log{n})$ time is a bit
tricky, since this is a divide and conquer algorithm. There are two ways we can
go about this.

<Spoiler title="Intuitive Proof">

Let's consider the following two facts:
1. In each `mergeSort` call, we process each element in the subarray exactly once.
2. The maximum recursion depth is $\mathcal{O}(\log{n})$.

The first fact is true because we merge the two subarrays in $\mathcal{O}(n)$ time.
To understand the second fact, we simply need to remember that we are halving 
our current array when splitting the array.

Thus, each element in the array is processed $\mathcal{O}(\log{n})$ times, resulting
in a runtime of $\mathcal{O}(n \log{n})$.

</Spoiler>

<Spoiler title="Formal Proof">

We can write the number of operations done as a function of the input.

$$T(n) = 2T(\frac{n}{2}) + n$$

Per the [Master Theorem](https://brilliant.org/wiki/master-theorem/),
we find $T(n) = \mathcal{O}(n \log{n})$.

</Spoiler>

## Quick Sort

Quick sort, like merge sort, is a divide and conquer sorting algorithm.
We use the following procedure:
- Choose a **pivot** element (arbitrarily chosen)
- Move all the elements less than or equal to our pivot to the left side
- Move all the elements greater than our pivot to the right side
- Recursively sort the left and right halves

We don't need to know how to code up 
quick sort; we just need to know how it
roughly works.

| Case             | Time Complexity        | Explanation                                           |
|------------------|------------------------|-------------------------------------------------------|
| Best Case        | $\mathcal{O}(n \log n)$ | Balanced splits lead to efficient sorting.            |
| Average Case     | $\mathcal{O}(n \log n)$ | Generally balanced partitioning.                      |
| Worst Case       | $\mathcal{O}(n^{2})$    | Unbalanced splits when pivot is poorly chosen.        |
| Space Complexity | $\mathcal{O}(\log n)$   | In-place sorting, but requires recursion stack.       |

In the best case scenario, our pivot choice splits our array into two roughly
equal subarrays. However, it is possible
that we choose the biggest or smallest element as our pivot, leading to a runtime of $\mathcal{O}(n^{2})$.
